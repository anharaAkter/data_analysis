{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7StMgLmh_Sq-"
      },
      "source": [
        "# Atelier BigData IA\n",
        "\n",
        "## Challenge - Kickstarter preprocessing\n",
        "---\n",
        "In this exercise, we will start working on the Kickstarter dataset, each record is about a specific campaign. Today, you will pre-process the dataset.\n",
        "\n",
        "During the next Spark course, you will apply machine learning to predict successful campaigns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T11:32:32.573002Z",
          "start_time": "2019-05-24T11:32:29.195373Z"
        },
        "id": "NQrtAqQP_SrA"
      },
      "outputs": [],
      "source": [
        "# Q0 - download csv file\n",
        "!curl -O https://s3.eu-central-1.amazonaws.com/alex-image-hosting/train_clean.csv > train_clean.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-iqpHWb_SrB"
      },
      "source": [
        "**Q1 - Have a look at our train_clean.csv file, with the linux ```head``` command.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T11:32:32.975499Z",
          "start_time": "2019-05-24T11:32:32.846049Z"
        },
        "id": "Ie40VnPJ_SrC"
      },
      "outputs": [],
      "source": [
        "# Q1 - Unix command for file first lines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELg23dcR_SrC"
      },
      "source": [
        "**Q2 - Create the spark session variable, name it \"preprocessing\".**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T08:51:01.320927Z",
          "start_time": "2019-05-24T08:50:54.625009Z"
        },
        "id": "-wHyD6oR_SrC"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoKspQNZ_SrC"
      },
      "source": [
        "## Loading & exploring data\n",
        "\n",
        "**Q3 - Load the data from the train_clean.csv. We have seen at Q1 that this file has a header.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T08:51:07.676814Z",
          "start_time": "2019-05-24T08:51:02.066030Z"
        },
        "id": "42dFJTXJ_SrC"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajTSN_h_SrC"
      },
      "source": [
        "**Q4 - Let's go for some exploration :**\n",
        "\t- 4.1) Number of lines and columns.\n",
        "\t- 4.2) Display the first 20  rows of the dataframe.\n",
        "\t- 4.3) Print the schema of the dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T08:51:09.447397Z",
          "start_time": "2019-05-24T08:51:07.680402Z"
        },
        "id": "1iEYEtaH_SrD"
      },
      "outputs": [],
      "source": [
        "# Q4.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T08:51:12.513613Z",
          "start_time": "2019-05-24T08:51:12.156389Z"
        },
        "scrolled": false,
        "id": "E7DEUpaM_SrD"
      },
      "outputs": [],
      "source": [
        "# Q4.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T08:51:14.070431Z",
          "start_time": "2019-05-24T08:51:14.056905Z"
        },
        "scrolled": true,
        "id": "pw9xqblT_SrD"
      },
      "outputs": [],
      "source": [
        "# Q4.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fpc2J0C_SrD"
      },
      "source": [
        "**Q5 - When printing the schema, we see that all columns are strings. Assign the integer type to columns you think appropriate. Have a look at the csv file. This new dataframe will be named dfCasted, print its schema.**\n",
        "\n",
        "*Hint : Use the .withColumn(newColName, newColValue) to cast each column.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EUA7Uz-_SrD"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifYMbbel_SrD"
      },
      "source": [
        "```\n",
        "root\n",
        " |-- project_id: string (nullable = true)\n",
        " |-- name: string (nullable = true)\n",
        " |-- desc: string (nullable = true)\n",
        " |-- goal: integer (nullable = true)\n",
        " |-- keywords: string (nullable = true)\n",
        " |-- disable_communication: string (nullable = true)\n",
        " |-- country: string (nullable = true)\n",
        " |-- currency: string (nullable = true)\n",
        " |-- deadline: integer (nullable = true)\n",
        " |-- state_changed_at: integer (nullable = true)\n",
        " |-- created_at: integer (nullable = true)\n",
        " |-- launched_at: integer (nullable = true)\n",
        " |-- backers_count: integer (nullable = true)\n",
        " |-- final_status: integer (nullable = true)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiELkb7g_SrH"
      },
      "source": [
        "**Q6 - We could have done this much faster. Do you know how ?**\n",
        "\n",
        "**Hint** : Have a look at parameters in cell n°4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiJABtZA_SrH"
      },
      "outputs": [],
      "source": [
        "# TODO : Write your answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSxp0yAm_SrH"
      },
      "source": [
        "## Data Cleaning\n",
        "\n",
        "**Q7 - Give a statistical description of these columns together : goal, backers_count, final_status**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T08:51:20.369650Z",
          "start_time": "2019-05-24T08:51:18.922089Z"
        },
        "id": "6f_P4v7s_SrH"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBlu4Hjj_SrH"
      },
      "source": [
        "**Q8 - Let's have a look at the **disable_communication** column. Group by values and display a descending value count. Show the top 10 values.**\n",
        "\n",
        "*Hint : groupBy, count, orderBy, show*\n",
        "\n",
        "What do you notice ? Considering the number of lines of our dataset, does this column provides information ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T08:51:23.635163Z",
          "start_time": "2019-05-24T08:51:21.334821Z"
        },
        "id": "gMcTXfWR_SrI"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VKboGnlP_SrI"
      },
      "source": [
        "**What shall you do with this **disable_communication** column ?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfbVvb0N_SrI"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxcQoWu7_SrI"
      },
      "source": [
        "**Q9 - Houston, we have a problem ! We can see the future in our dataset ! Can you find it ? These informations must be removed.**\n",
        "\n",
        "*Hint : There are two problematic columns, it has something to do with the supporters, and a change during the project.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T08:52:23.559663Z",
          "start_time": "2019-05-24T08:52:23.520082Z"
        },
        "id": "GVm0sIRP_SrI"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCNO-IxK_SrI"
      },
      "source": [
        "**Q10 - Country & Currency : Start with some exploration of these columns.**\n",
        "\n",
        "- Try some groupBy and counting, just like *Q8*. Then, read below.\n",
        "\n",
        "You may think that *country* and *currency* are redundant, in which case we could just delete one of the two columns. What about Euro ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T08:54:45.422336Z",
          "start_time": "2019-05-24T08:54:44.247494Z"
        },
        "id": "D_3xNRoM_SrI"
      },
      "outputs": [],
      "source": [
        "# TODO : Country value count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T08:54:36.206107Z",
          "start_time": "2019-05-24T08:54:35.227295Z"
        },
        "id": "BUsXdtIx_SrI"
      },
      "outputs": [],
      "source": [
        "# TODO : Currency value count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyvapAet_SrI"
      },
      "source": [
        "- Try selecting *goal* and *final_status*, and show some values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T08:57:34.730321Z",
          "start_time": "2019-05-24T08:57:34.598722Z"
        },
        "scrolled": true,
        "id": "5S0h4ClJ_SrI"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgkQ6IuZ_SrI"
      },
      "source": [
        "- Try showing value count for country and currency in the same table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T08:58:03.016253Z",
          "start_time": "2019-05-24T08:58:01.752252Z"
        },
        "id": "2O3evSKx_SrJ"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfxD7UVN_SrJ"
      },
      "source": [
        "**Q11 - Now, there is something else : Some values for *country* have the value *False*. Display these records, and groupBy *currency*, descending.**\n",
        "\n",
        "*Hint : The instruction chain is the following : dfCasted.filter().groupBy().count().orderBy().show(), fill 3 parentheses.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T09:00:27.287483Z",
          "start_time": "2019-05-24T09:00:26.307833Z"
        },
        "id": "P0MPWKjH_SrJ"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDG7MaHY_SrJ"
      },
      "source": [
        "*Definition - Custom functions :* Some column operations are already defined inside Spark, but we often need to apply more complex or more custom function. In this case, we can create User Defined Functions (UDF) and apply them on columns.\n",
        "\n",
        "**Q12 - In this question, we will create two UDF.**\n",
        "- **udfCountry(country, currency)** : If country=False, take the currency value, else, leave the country value.\n",
        "- **udfCurrency(currency)** : If the length of currency is different than 3, assign a null value, else, leave the currency value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T09:12:38.443793Z",
          "start_time": "2019-05-24T09:12:38.423164Z"
        },
        "id": "M3EeLBtf_SrJ"
      },
      "outputs": [],
      "source": [
        "# Q12 - UDF\n",
        "\n",
        "# Convert the functions to Spark UDF types : udf(pythonFunction, outputType())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff7pV7Dx_SrJ"
      },
      "source": [
        "- BinaryType – Données binaires.\n",
        "- BooleanType – Valeurs booléennes.\n",
        "- ByteType – Valeur d'octet.\n",
        "- DateType – Valeur d'horodatage.\n",
        "- DoubleType – Valeur double à virgule flottante.\n",
        "- IntegerType – Valeur d'entier.\n",
        "- LongType – Valeur d'entier long.\n",
        "- NullType – Valeur null.\n",
        "- ShortType – Valeur d'entier court.\n",
        "- StringType – Chaîne de texte.\n",
        "- TimestampType – Valeur d'horodatage (généralement en secondes à partir du 01/01/1970).\n",
        "- UnknownType – Valeur de type non identifié."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPsM6cqV_SrJ"
      },
      "source": [
        "**Q13 - In this question we will apply our two UDF. Using the .withColumn operation, you can change a column, just like you did for type casting. withColumn will create two new columns : country2 and currency2.**\n",
        "\n",
        "*Hint : df.withcolumn(country2, newColValue).withcolumn(currency2, newColValue)*\n",
        "\n",
        "Also, you can add a drop statement (on country and currency) after the two withColumns, as we have created our new columns.\n",
        "\n",
        "Check your dataframe once transformations are applied. Schema and first lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOeU043c_SrJ"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOhFv8Ya_Sra"
      },
      "source": [
        "**Q14 - We will do one more cleanup on the column final_status**, which will be the label for our classification algorithm in next course.\n",
        "\n",
        "First, count the number of elements for each values in final_status.\n",
        "\n",
        "Finally, we need to delete records with **final_status** different than 0 or 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T09:16:00.989530Z",
          "start_time": "2019-05-24T09:15:59.871903Z"
        },
        "id": "huccU1PZ_Sra"
      },
      "outputs": [],
      "source": [
        "# TODO : final_status count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T09:23:41.623679Z",
          "start_time": "2019-05-24T09:23:41.603265Z"
        },
        "id": "H9YTQ_hZ_Sra"
      },
      "outputs": [],
      "source": [
        "# TODO : filter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T09:23:45.598338Z",
          "start_time": "2019-05-24T09:23:44.718137Z"
        },
        "id": "gq7lTmKI_Sra"
      },
      "outputs": [],
      "source": [
        "# TODO : Check processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKTmsLQ9_Sra"
      },
      "source": [
        "## Feature engineering\n",
        "\n",
        "It's sometimes useful to add features to our dataframe, to help our model learning. We will work with the time data.\n",
        "\n",
        "**Q15 - Our dates columns are in unix timestamps. We first need to convert them to dates.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T10:19:23.071035Z",
          "start_time": "2019-05-24T10:19:22.782920Z"
        },
        "id": "W2Ahn6XY_Sra"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rVZ0jIN_Sra"
      },
      "source": [
        "**Q16 - Add a **days_campaign** column, which represents the duration of the campaign, in days. This is the difference between *launched_at* and *deadline*. Here we work with a date difference.**\n",
        "\n",
        "Add a **hours_prep**, which represents the number of hours of preparation. This is the difference between *created_at* and *launched_at*. You may round to 2 digits after comma. Here we work with a timestamp difference.\n",
        "\n",
        "Finally, apply a filter : we want to delete records with **days_campaign** AND **hours_prep** equal to zero, and we want the records with **goal** greater than zero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T10:29:43.174499Z",
          "start_time": "2019-05-24T10:29:41.798121Z"
        },
        "id": "a1p-aW92_Sra"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import round, datediff\n",
        "\n",
        "# Date difference : datediff(Col1, Col2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T10:31:15.124747Z",
          "start_time": "2019-05-24T10:31:14.897901Z"
        },
        "scrolled": true,
        "id": "ZQ8M2iod_Srb"
      },
      "outputs": [],
      "source": [
        "# TODO : Filter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO3Fm4lQ_Srb"
      },
      "source": [
        "**Q17 - At this point, we don't need these columns anymore : *created_at*, *launched_at*, *deadline*.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T10:31:15.467978Z",
          "start_time": "2019-05-24T10:31:15.449766Z"
        },
        "id": "wohzuOop_Srb"
      },
      "outputs": [],
      "source": [
        "# TODO : Drops"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CHpQtGSi_Srb"
      },
      "source": [
        "We will now work on text data, we will gather every text values into one.\n",
        "\n",
        "**Q18 - Pass the columns *name*, *desc* and *keywords* into lowercase.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T10:31:16.088184Z",
          "start_time": "2019-05-24T10:31:16.054039Z"
        },
        "id": "kjwrE5Z4_Srb"
      },
      "outputs": [],
      "source": [
        "# A little search for passing strings to lower case ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w24kIm-m_Srb"
      },
      "source": [
        "**Q19 - Create a new column called *text* which contains the three previous columns. Be careful to include a space between them so that we can split them later.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPxFzvjC_Srb"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import concat_ws\n",
        "\n",
        "# Hint : Google(\"pyspark concat_ws\"), don't forget the separator parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zppAUj28_Srb"
      },
      "source": [
        "**Q20 - You can now delete these three text columns.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T10:31:18.118153Z",
          "start_time": "2019-05-24T10:31:18.101652Z"
        },
        "id": "TolHbSzt_Srb"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8Eij5wV_Srb"
      },
      "source": [
        "## Processing null values\n",
        "\n",
        "**Q21 - There are various techniques to handle null values to make them usable by an algorithm. Can you find 3 different methods ?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZICKyAx_Srb"
      },
      "outputs": [],
      "source": [
        "# TODO : Write your answer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqI2cjIP_Src"
      },
      "source": [
        "**Q22 - For the columns *days_campaign*, *hours_prep* and *goal* : replace null values by **-1**.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T10:37:16.344695Z",
          "start_time": "2019-05-24T10:37:16.302203Z"
        },
        "id": "ar5jNudF_Src"
      },
      "outputs": [],
      "source": [
        "# Look for na.fill at the following adress :\n",
        "# https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html\n",
        "\n",
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XwSuvZ4_Src"
      },
      "source": [
        "**Q23 - For the columns **country2** and **currency2** : replace null values by **\"unknown\"**.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cdGy1Q2_Src"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqvXFub3_Src"
      },
      "source": [
        "## Exporting Dataframe\n",
        "\n",
        "Well done, you have done a pretty good pipeline for pre-processing your dataset.\n",
        "\n",
        "**Q24 - Finally, export your dataframe to the *parquet* format.**\n",
        "\n",
        "*parquet* always exports a folder that may contain multiple files, this is due to the distributed nature of Spark.\n",
        "\n",
        "The export function creates a directory with the name given in parameter. Give it **\"kickstarter.parquet\"**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-05-24T10:40:48.532087Z",
          "start_time": "2019-05-24T10:40:45.353047Z"
        },
        "id": "178oRmMs_Src"
      },
      "outputs": [],
      "source": [
        "# TODO"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": false,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}